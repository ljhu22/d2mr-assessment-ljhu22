---
title: 'Level 2 Data Cleaning: Clean the midwest Dataset'
---

# Objective

The objective of this assignment is to practice cleaning and transforming a messy dataset using tidyverse functions. You will use skills like renaming and reordering columns, sorting rows, changing data types, mutating data, and using the stringr and forcats packages.

This is the Level 2 Data Cleaning assignment. You may additionally or alternatively complete the [Level 1 Data Cleaning assignment](https://github.com/nrdowling/d2mr-assessment/tree/main/01_data-cleaning/01_cleaning-level-1), in which you will work with a simple dataset and focus on basic data cleaning tasks. The Level 1 assignment has more direct instruction and is recommended for those who are new to data cleaning.

In this Level 2 Cleaning assignment, you will work with a more complex dataset and perform additional cleaning tasks with less direct instruction. The Level 2 assignment has more opportunities to demonstrating meeting course standards than this Level 1 assignment and is recommended for those who are already comfortable with the tasks in this assignment.

# Instructions

1. If you have not already done so, pull the latest changes from the `d2mr-assessment` repository to ensure you have the most up-to-date version of the assignment files. Confirm you are working in your fork of the repository.
2. Open `cleaning-level-2.qmd` in RStudio and follow the instructions in the Setup section below to load and inspect the (original) `midwest` dataset. 
    - **Important:** Unlike Level 1, you will not be provided with a goal dataset to match. Instead, you will evaluate what cleaning tasks are necessary or useful *in principle*. You can reference the original `midwest` dataset, but ultimately you will need to decide what the "clean" version of the dataset should look like.
3. Follow the guideline to identify and perform cleaning tasks on the `messy-midwest.csv` dataset.
4. At some points in this document you may come across questions or non-coding exercises. Answer these questions in the text of this .qmd document, immediately below the question.
5. *Optional:* Continue to follow the instructions in the assignment script to clean the dataset above and beyond matching the original. 

# Setup

Run these chunks as written. Do not make changes to code except where noted if necessary.

## Loading libraries and set seed

```{r}
#| label: setup
library(tidyverse)
set.seed(1234)
```


## Read in and inspect messy data

Read in and inspect the messy dataset `messy-midwest.csv`.

```{r}

#| label: read-messy-data

### LEAVE THIS CHUNK AS-IS ###

# You *might* need to edit the filepath, but don't change anything else!

# Read in messy-midwest.csv
messy.midwest <- read_csv(
  ########################################
  "messy-midwest.csv", ## <-- THIS IS THE ONLY THING IN THIS CHUNK YOU CAN CHANGE IF NECESSARY
  ########################################
  trim_ws = FALSE, name_repair = "minimal", col_types = cols(.default = col_character()))

# Inspect the structure and contents of the messy midwest dataset with head(), glimpse(), str(), and/or View()
head(messy.midwest)
glimpse(messy.midwest)
str(messy.midwest)
View(messy.midwest)

```

## Inspect the original midwest dataset

```{r}
#| label: inspect-original-data

### LEAVE THIS CHUNK AS-IS ###

# Load the original midwest dataset
data(midwest)

# View the documentation for the midwest dataset
?midwest

# Inspect the structure and contents original midwest dataset with head(), glimpse(), str(), and/or View()
head(midwest)
glimpse(midwest)
str(midwest)
View(midwest)

```

QUESTIONS:

1. What are the differences between the messy dataset and the original midwest dataset?
<!-- answer below -->
1) Column Order: The columns in the messy dataset are arranged differently from the original dataset.
2) Column Name Discrepancies: There are several column name issues in the messy dataset, such as "C0unty Name" instead of "County Name" and "Total P0pulation" instead of "Total Population". Some columns are also missing from one dataset but present in the other.
3) Data Types: In the messy dataset, most of the columns are stored as text, while in the original dataset, many of these should be numeric.
4) Inconsistent Values: The messy dataset has issues with inconsistent values, like misspelled county names, extra spaces, and capitalization inconsistencies.
5) Number Formatting: There are inconsistencies in the way numbers are formatted, such as irregular decimal places and potential issues with commas versus periods.
6) Rounding Differences: The numbers in the messy dataset are not rounded consistently, unlike in the original dataset where the rounding is more standardized.


2. What are the biggest issues you need to address in cleaning?
<!-- answer below -->
1) Rename columns: Several column names have spelling errors, incorrect characters (like "0" instead of "O"), and mismatched names between the two datasets.
2) Data Type Issues: Many columns that should be numeric are currently stored as text. These need to be converted to the appropriate data types (e.g., population counts and percentages should be numeric) for accurate analysis.
3) Inconsistent and Misspelled Values: Some county names are misspelled or have extra spaces and formatting issues. These need to be corrected to ensure consistency across the dataset.
4) Irregular Number Formatting: Numbers in the dataset are not consistently formatted, with potential issues like commas instead of periods, as well as different decimal places. This should be addressed to maintain uniformity.
5) Rounding Differences: The numbers in the messy dataset have varying decimal places, while the original dataset uses more consistent rounding. Standardizing the number of decimal places is necessary for better data consistency.

3. Are there any differences between the messy dataset and the original dataset that you do not need or want to address in cleaning? If so, why (not)?
<!-- answer below -->!
1) Additional Columns in the Messy Dataset: The messy dataset contains a few additional columns, such as "Population per Sq Mile" and "Population Over 18." These extra columns do not need to be removed, because they may contain useful data.
2) The "Category" Column: Both datasets have a "category" column, but this column might not need much adjustment. If the values in this column are consistent across both datasets, no action is required.


4. Are there additional cleaning tasks you would like to perform beyond matching the original dataset? If so, what are they and why do you think they are important?
<!-- answer below -->





# Cleaning

You may approach cleaning the dataset however you like based on how you identified problems above and how you think they should be prioritized.

If you're not sure where to start, you can organize your cleaning into the following categories. **You do not need to follow this structure.** Feel free to delete these sections, rearrange them, or add new ones as needed. (FYI: When I cleaned this myself I loosely followed this structure, but there were some parts of my approach that could not have worked in this order.)

You can additionally/alternatively construct your cleaning in a single pipeline in the last chunk.

## Selecting, renaming, and reordering columns

```{r}
#load the dplyr package
library(dplyr)

# Check column names in the messy dataset
colnames(messy.midwest)


# 1. Selecting relevant columns
selected_columns <- c("C0unty Name", "State", "Area (sq miles)", "Total P0pulation", 
                      "Population Density", "Percentage Asian", "Population Over 18", 
                      "Percentage College", "Percentage Below Poverty")

# 2. Renaming columns to match the original dataset
messy_cleaned <- messy.midwest %>%
  rename(
    county = `C0unty Name`,
    state = State,
    area = `Area (sq miles)`,
    poptotal = `Total P0pulation`,
    popdensity = `Population Density`,
    percasian = `Percentage Asian`,
    popadults = `Population Over 18`,
    percollege = `Percentage College`,
    percbelowpoverty = `Percentage Below Poverty`
  )

# View the structure of the cleaned dataset
str(messy_cleaned)

# 3. Reordering columns to match the structure
messy_cleaned <- messy_cleaned %>%
  select(PID, county, state, area, poptotal, popdensity, popwhite, popblack, popamerindian, popasian, popother, percamerindan, percasian, percother, popadults, perchsd, percollege, percprof, poppovertyknown, percpovertyknown, percbelowpoverty, percchildbelowpovert, percadultpoverty, percelderlypoverty, inmetro, category)

# View the cleaned dataset
head(messy_cleaned)

```

## Changing data types

```{r}
#Check the data type of each variable before changing the data types
str(messy_cleaned)

# Changing data types
messy_cleaned <- messy_cleaned %>%
  
# Convert numeric columns from character to numeric
 mutate(
    area = as.numeric(area),
    popdensity = as.numeric(popdensity),
    poptotal = as.integer(poptotal),
    popwhite = as.integer(popwhite),
    popblack = as.integer(popblack),
    popamerindian = as.integer(popamerindian),
    popasian = as.integer(popasian),
    popother = as.integer(popother),
    percamerindan = as.numeric(percamerindan),
    percother = as.numeric(percother),
    popadults = as.integer(popadults),
    perchsd = as.numeric(perchsd),
    percprof = as.numeric(percprof),
    poppovertyknown = as.integer(poppovertyknown),
    percpovertyknown = as.numeric(percpovertyknown),
    percchildbelowpovert = as.numeric(percchildbelowpovert),
  )

# Check the structure of the data after changing the data types
str(messy_cleaned)

```


## Mutating data

```{r}
#| label: Log transform specific variable
# Log transform the population total
messy_cleaned <- messy_cleaned %>%
  mutate(
    log_popdensity = log(popdensity)
  )

# View the new log-transformed column
head(messy_cleaned)

#| label: Create a new column
# Create a new variable 'poverty_status' based on 'percbelowpoverty'
messy_cleaned <- messy_cleaned %>%
  mutate(
    poverty_status = ifelse(percbelowpoverty > 20, "High Poverty", "Low Poverty")
  )

# View the new column
head(messy_cleaned)

#| label: Categorize a variable into levels
# Categorize population density into different levels
messy_cleaned <- messy_cleaned %>%
  mutate(
    pop_density_category = case_when(
      popdensity < 500 ~ "Low",
      popdensity >= 500 & popdensity <= 2000 ~ "Medium",
      popdensity > 2000 ~ "High",
      TRUE ~ "Unknown"
    )
  )

# View the new column
head(messy_cleaned)

```


## Using stringr and forcats

```{r}
#Load the stringr package
library(stringr)

#| USING stringr
#| label: Trimming Leading and Trailing Whitespace from the 'county' Column
# Clean up by removing any extra spaces at the start or end of the county names
messy_cleaned <- messy_cleaned %>%
  mutate(county = str_trim(county))

# View the cleaned column
head(messy_cleaned$county)

#| label: Replacing Text in the 'county' Column
# Replace any instances of "County" with an empty string
messy_cleaned <- messy_cleaned %>%
  mutate(county = str_replace(county, "County", ""))

# View the modified 'county' column
head(messy_cleaned$county)

#| label:Converting All Text to Uppercase
# Convert the 'county' column to uppercase
messy_cleaned <- messy_cleaned %>%
  mutate(county = str_to_upper(county))

# View the cleaned 'county' column
head(messy_cleaned$county)


#| USING forcats
#| label: Reordering the Factor Levels of the 'category' Column
# Rearrange categories by their frequency of appearance
messy_cleaned <- messy_cleaned %>%
  mutate(category = fct_infreq(category))

# view the reordered 'category' column
head(messy_cleaned$category)


```


## Other cleaning tasks

```{r}
#| label: Cleaning Missing Data
#Remove rows with missing values
messy_cleaned <- messy_cleaned %>%
  drop_na()

# View the cleaned dataset
head(messy_cleaned)

#| label:Removing Duplicates
# Remove duplicate rows on all columns
messy_cleaned <- messy_cleaned %>%
  distinct()

# View the cleaned dataset
head(messy_cleaned)

#| label:Removing Outliers
#Remove outliers of the Population Density
# Remove rows where 'popdensity' is greater than 10000
messy_cleaned <- messy_cleaned %>%
  filter(popdensity < 10000)

# Remove rows where 'popdensity' is smaller than 400
messy_cleaned <- messy_cleaned %>%
  filter(popdensity > 400)

# View the cleaned dataset
head(messy_cleaned)

#| Formatting and Standardizing Columns
#capitalizing the first Letter of each word in a country column
messy_cleaned <- messy_cleaned %>%
  mutate(county = str_to_title(county))

# View the updated 'county' column
head(messy_cleaned$county)


```




## All cleaning in a single pipeline
```{r}

#| label: one-pipeline

library(dplyr)
library(stringr)

# Perform all cleaning tasks in a single pipeline
messy_cleaned <- messy_cleaned %>%
  # Remove rows with missing values
  drop_na() %>%
  
  # Remove duplicate rows
  distinct() %>%
  
  # Remove outliers in 'popdensity' (greater than 10000 and less than 400)
  filter(popdensity < 10000, popdensity > 400) %>%
  
  # Capitalize the first letter of each word in 'county' column
  mutate(county = str_to_title(county))

# View the cleaned dataset
head(messy_cleaned)

```
ban
# Reflection

QUESTIONS:

1. Is your dataset identical to `midwest`? If not, what is different? (Remember the functions `all.equal()` and `diff_data()` can help here.)

NO, they are not identical.

1) Missing Data: I removed rows with missing values. Any rows with missing data in any column were excluded, making the cleaned dataset smaller than the original.
2) Duplicates: Duplicate rows were removed from the dataset. If the original dataset had any repeated rows, they were eliminated in the cleaned dataset.
3) Outliers: I filtered out rows where popdensity was greater than 10,000 or less than 400. This removed extreme values that were considered outliers, leading to a difference from the original dataset if those values were present.
4) Text Formatting:
  4.1)Trimming Whitespace: I removed extra spaces from the county column. Any leading or trailing spaces were eliminated.
  4.2)Replacing "County": I replaced the word "County" with an empty string in the county column. This changed how county names were represented.
  4.3)Uppercase Transformation: I converted the county column to uppercase. This transformation ensured consistency in the text format but deviated from the original midwest dataset, which likely had different capitalization.
5)Creating New Columns:
  5.1)poverty_status: I created a new column based on percbelowpoverty, categorizing areas as either "High Poverty" or "Low Poverty" based on whether the percentage was greater than 20. This column wasn't present in the original dataset.
  5.2)pop_density_category: I categorized popdensity into three categories ("Low", "Medium", "High", and "Unknown") using case_when(). This categorization wasn’t in the original dataset.
6)Log Transformation: I applied a log transformation to the popdensity column and created a new column log_popdensity. This transformation wasn’t part of the original dataset.
7) Data Type Conversion: I converted several columns from character (chr) to numeric or integer. This included columns like popdensity, poptotal, popwhite, popblack, etc., ensuring the correct data types for analysis.
8) Reordering Factor Levels: I reordered the levels of the category column. The original dataset likely had different factor levels and orderings.


2. Did you make any choices to clean the data that resulted in the dataset not matching the original? Why did you make those choices?

Yes, several choices were made that caused the cleaned dataset to differ from the original.
1)Removing Missing Data: Rows with missing values were removed. This was done to ensure the dataset was complete, but it reduced the size of the dataset compared to the original.
Removing Duplicates: Duplicate rows were removed to avoid redundancy in the dataset. This resulted in a cleaned dataset that is not identical to the original if duplicates existed.
2)Outlier Removal: Extreme values of popdensity (greater than 10,000 or less than 400) were filtered out because such outliers could distort the analysis. This was done to make the data more meaningful for analysis, but it led to a difference from the original dataset.
3)Text Formatting Changes: The county column was changed to uppercase and had "County" removed. These changes were made for consistency and readability, but they differed from the original dataset’s text formatting.
4)Log Transformation: A log transformation was applied to popdensity to handle skewness in the data. This altered the dataset compared to the original, which did not have this transformation.
5)New Column Creation: The poverty_status and pop_density_category columns were added based on existing data to categorize areas by poverty level and population density. These transformations were not present in the original dataset.


3. Were there any cleaning steps -- whether necessary to recreate the original df or just because you wanted to do them -- that you weren't able to implement? If so, what were they and what more would you need to do/know to implement them? 

There are some things that could be adjusted to make the cleaned dataset match the original more closely
1) Reintroducing Missing Data: To match the original dataset exactly, I would need to retain rows with missing data instead of removing them. Alternatively, I could impute missing values, perhaps using the mean or median to fill in the gaps.
2)Preserving Duplicates: If the original dataset contained duplicates that were meaningful, I would need to ensure those duplicates are kept.


# Unguided cleaning and transformation

*Optional:* If you have the time and interest, continue transforming this dataset as you please. Create new columns based on the existing ones, reformat strings, try your hand at a regex replacement, summarize by groups (factor levels), visualize a simple relationship, or anything else you can think of. To get you started, consider things like:

##Extra data cleaning
```{r}
#| label:Fix the typo of the County
 
library(dplyr)
library(stringr)

#rename the county to country
messy_cleaned <- messy_cleaned %>%
  rename(country = county)

# Replace numbers with corresponding letters of County
replace_numbers <- function(input_string) {
  input_string %>%
    str_replace_all("0", "o") %>%
    str_replace_all("1", "l") %>%
    str_replace_all("3", "e")
}

#Clean the country name
messy_cleaned <- messy_cleaned %>%
  mutate(country = replace_numbers(country)) %>%
  mutate(country = str_to_title(country)) 
  
#Clean the state names
 mutate(state = case_when(
    state %in% c("Ill.", "Illinois") ~ "IL",
    state %in% c("Mich.", "Michigan") ~ "MI",
    state %in% c("Wis.", "Wisconsin") ~ "WI",
    state %in% c("Ind.", "Indiana") ~ "IN",
    state %in% c("OHIO", "Ohio") ~ "OH",
    TRUE ~ state
  ))

# View the cleaned dataset
head(messy_cleaned)

```



1. **Exploratory Data Analysis:** Use the cleaned dataset to explore relationships between variables, create visualizations, and generate insights.

```{r}

#| 1. label: Create Visualizations 1.0

#load the package
library(dplyr)
library(ggplot2)
install.packages(corrplot)
library(corrplot)

# View the statistics summary and the structure
summary(messy_cleaned)
str(messy_cleaned)

# 1.1 Plotting the pie chart of each ehicity group
ethnicity_data <- messy_cleaned %>%
  select(popwhite, popblack, popamerindian, popasian, popother) %>%
  summarise(across(everything(), sum, na.rm = TRUE)) %>%
  pivot_longer(everything(), names_to = "Ethnicity", values_to = "Population")

ggplot(ethnicity_data, aes(x = "", y = Population, fill = Ethnicity)) +
  geom_bar(stat = "identity", width = 1) + 
  coord_polar(theta = "y") + 
  theme_void() + 
  labs(title = "Ethnicity Distribution") + 
  theme(legend.title = element_blank(), 
        legend.position = "right")

# 1.2 Create histograms for each state grouped by 'country'
ggplot(messy_cleaned, aes(x = popdensity)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  facet_wrap(~ state) +  # Create a separate plot for each state
  theme_minimal() +
  labs(title = "Population Density Distribution by State", 
       x = "Population Density", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 


# 1.3 Plot histogram of population density
ggplot(messy_cleaned, aes(x = popdensity)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Population Density", x = "Population Density", y = "Frequency")


# 1.4Scatter plot to show the relationship between 'popdensity' and 'poptotal'
ggplot(messy_cleaned, aes(x = popdensity, y = poptotal)) +
  geom_point(color = "red") +
  theme_minimal() +
  labs(title = "Population Density vs. Total Population", x = "Population Density", y = "Total Population")


#| 2. label: Find Corrleation & Insights 
library(tidyverse)

# Select numeric columns
correlation_data <- messy_cleaned %>%
  select_if(is.numeric)

# Calculate the correlation matrix
correlation_matrix <- cor(correlation_data, use = "complete.obs")

# Generte the correlation matrix into a data frame
correlation_df <- as.data.frame(as.table(correlation_matrix))

# Filter for strong correlations
strong_correlations <- correlation_df %>%
  filter(abs(Freq) > 0.8 & Var1 != Var2)  # abs() to get both positive and negative correlations

#| 3. label: Create Visualizations 2.0

# 3.1 Boxplot to show population density by poverty status
ggplot(messy_cleaned, aes(x = poverty_status, y = popdensity, fill = poverty_status)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Population Density by Poverty Status", 
       x = "Poverty Status", y = "Population Density") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 3.2 Violin plot for distribution of population total by region category
ggplot(messy_cleaned, aes(x = category, y = poptotal, fill = category)) +
  geom_violin() +
  theme_minimal() +
  labs(title = "Population Total by Region Category", 
       x = "Region Category", y = "Population Total") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 3.3 Scatter plot of population density vs. total population, grouped by region category
ggplot(messy_cleaned, aes(x = popdensity, y = poptotal)) +
  geom_point(aes(color = category)) +
  facet_grid(~ category) + 
  theme_minimal() +
  labs(title = "Population Density vs. Total Population by Region", 
       x = "Population Density", y = "Total Population")

# 3.4 Plot the correlation heatmap of numeric variables
library(corrplot)
corrplot(correlation_matrix, method = "color", 
         type = "upper", 
         tl.cex = 0.8,
         addCoef.col = "black", 
         title = "Correlation Heatmap", 
         mar = c(0,0,1,0))

# 3.5 Bar plot of population proportions by ethnicity
ggplot(ethnicity_data, aes(x = Ethnicity, y = Population, fill = Ethnicity)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Population Proportions by Ethnicity", x = "Ethnicity", y = "Population") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 3.6 Bar plot of population total by poverty status
ggplot(messy_cleaned, aes(x = poverty_status, y = poptotal, fill = poverty_status)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Population Total by Poverty Status", 
       x = "Poverty Status", y = "Population Total")

# 3.7 Scatter plot to explore poverty percentage vs. population density
ggplot(messy_cleaned, aes(x = percpovertyknown, y = popdensity)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Poverty Percentage vs Population Density", 
       x = "Poverty Percentage", y = "Population Density")

#4.4 Mediation Model

install.packages(c("lm.beta", "pwr", "ggplot2", "segmented"))
library(lm.beta)
library(pwr)
library(ggplot2)
library(segmented)
library(readr)

# Changing data types
messy_cleaned <- messy_cleaned %>%
  mutate(
    percadultpoverty = as.numeric(percadultpoverty),
  )

# Find the Mediation role of the log_popdensity
source("/Users/linjinhu/Desktop/process.R")
process (data=messy_cleaned, y="percadultpoverty", x="percchildbelowpovert", m='log_popdensity',
         model=4, total=1, stand=1, boot=10000)

```


2. **Data Transformation:** Create new variables, aggregate data, or reshape the dataset to prepare it for analysis.

```{r}

#| 1. label: Create new variables
# 1.1 Create a categorical variable based on quantiles of population density
messy_cleaned <- messy_cleaned %>%
  mutate(pop_density_quantile = ntile(popdensity, 4))  

# 1.2 Create a new categorical variable for Urban vs. Rural based on population density
messy_cleaned <- messy_cleaned %>%
  mutate(urban_rural = case_when(
    popdensity > 1000 ~ "Urban",
    popdensity <= 1000 ~ "Rural",
    TRUE ~ "Unknown"
  ))

# 1.3 Create a new variable that sums population by ethnic group
messy_cleaned <- messy_cleaned %>%
  mutate(total_ethnicity_population = popwhite + popblack + popamerindian + popasian + popother)

# 1.4 Calculate the proportion of each ethnicity group
messy_cleaned <- messy_cleaned %>%
  mutate(
    prop_white = popwhite / poptotal,
    prop_black = popblack / poptotal,
    prop_amerindian = popamerindian / poptotal,
    prop_asian = popasian / poptotal,
    prop_other = popother / poptotal
  )

#| 2. label: Aggregating Data

#2.1 Summarize the total population by region or category
aggregated_data <- messy_cleaned %>%
  group_by(category) %>%
  summarise(total_population = sum(poptotal, na.rm = TRUE),
            avg_popdensity = mean(popdensity, na.rm = TRUE),
            median_popdensity = median(popdensity, na.rm = TRUE),
  ) 

#2.2 Summarize population and poverty data by category
aggregated_data2 <- messy_cleaned %>%
  group_by(category, poverty_status) %>%
  summarise(mean_poverty = mean(percpovertyknown, na.rm = TRUE),
            total_population = sum(poptotal, na.rm = TRUE),
            .groups = "drop")




```

3. **Split, Merge, and Reshape:** Split the dataset into multiple datasets or merge it with other datasets using `join` functions to create a new dataset. Use `pivot_longer()` and `pivot_wider()` to reshape the data.

```{r}

#| 1. label: Splitting the Dataset into Multiple Datasets
# 1.1 Splitting the data based on a categorical variable
category_split <- messy_cleaned %>%
  group_by(category) %>%
  group_split()

# 1.2 Split data by 'poverty_status' and 'category'
poverty_category_split <- messy_cleaned %>%
  group_by(poverty_status, category) %>%
  group_split()

#| 2. label:Merging Datasets
#2.1 Self join the dataset based on the 'category' column
self_joined_data <- messy_cleaned %>%
  left_join(messy_cleaned, by = "category", suffix = c("_left", "_right"))

# View the resulting data
head(self_joined_data)

# 2.2 Self join based on multiple columns: 'category' and 'state'
self_joined_multiple <- messy_cleaned %>%
  left_join(messy_cleaned, by = c("category", "state"), suffix = c("_left", "_right"))

# View the resulting data
head(self_joined_multiple)

#| 3. Reshape
# 3.1 Remove 'pop_density_category' and pivot only the population-related columns
long_data <- messy_cleaned %>%
  select(-pop_density_category) %>%  # Exclude the pop_density_category column
  pivot_longer(cols = starts_with("pop"), 
               names_to = "year", 
               values_to = "population")

# View the transformed data
head(long_data)

# 3.2 Pivoting data from long to wide format
wide_data <- messy_cleaned %>%
  pivot_wider(names_from = "year", values_from = "population")

# View the transformed data
head(wide_data)

# 3.3  Pivoting data using multiple key variables

# Check the columns that all with pop
head(messy_cleaned %>%
  select(starts_with("pop")))

# Pivot the data with multiple key variables and repair duplicated column names
multi_pivot_data <- messy_cleaned %>%
  select(-pop_density_category) %>%
  pivot_longer(cols = starts_with("pop"),
               names_to = c("year", "category"),
               names_sep = "_",
               values_to = "population",
               names_repair = "unique") 

# View the transformed data
head(multi_pivot_data)

```

4. **Informativity:** Consider the midwest data and its documentation. Clean/transform the dataframe into a format that is more informative, transparent, or easier to work with. For example, improve column naming conventions, create new (useful) variables, reduce redundancy, or eliminate variables that are not useful or documented.


# Submission & Assessment

To submit:

1. Add an `assessment.md` file to this mini-project's directory:
    1. Check off all objectives you believe you have demonstrated
    2. Indicate which unique objectives you are meeting for the first time (if any)
    3. Complete any relevant open-ended items
2. Push your changes to your centralized assignment repository on GitHub. 
3. Confirm that Dr. Dowling and your section TA are added as collaborators to your repository.
4. Submit your work in your next open mini-project assignment by including the following information in the text box:
    1. The title of the assignment: "Level 2 Data Cleaning: Clean the midwest Dataset"
    2. A link to the **directory** for this assignment in your centralized assignment repo



